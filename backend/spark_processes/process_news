from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, lower, regexp_replace, to_timestamp, when
from pyspark.sql.types import StringType, TimestampType
import os
from dotenv import load_dotenv

load_dotenv()

def create_spark_session():
    return SparkSession.builder \
        .appName("NewsArticleProcessor") \
        .config("spark.jars", "/path/to/mysql-connector-java.jar") \
        .getOrCreate()

def load_data_from_mysql(spark):
    return spark.read \
        .format("jdbc") \
        .option("url", f"jdbc:mysql://{os.getenv('DB_HOST')}/{os.getenv('DB_NAME')}") \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .option("dbtable", "articles") \
        .option("user", os.getenv('DB_USER')) \
        .option("password", os.getenv('DB_PASSWORD')) \
        .load()

def clean_text(text):
    # Remove special characters, convert to lowercase
    return regexp_replace(lower(text), "[^a-zA-Z0-9\\s]", "")

def process_articles(df):
    clean_title_udf = udf(clean_text, StringType())
    df = df.withColumn("cleaned_title", clean_title_udf(df.title))
    
    # Normalize source names
    df = df.withColumn("normalized_source", 
                       when(df.source == "24chasa", "24 Chasa")
                       .when(df.source == "Dnevnik", "Dnevnik")
                       .when(df.source == "Fakti", "Fakti")
                       .otherwise(df.source))
    
    # Convert published to timestamp if it's not already
    df = df.withColumn("published", to_timestamp(df.published))
    
    # Add day of week
    df = df.withColumn("day_of_week", date_format(df.published, "E"))
    
    return df

def analyze_articles(df):
    # Count articles by source
    source_counts = df.groupBy("normalized_source").count().orderBy("count", ascending=False)
    
    # Count articles by day of week
    day_counts = df.groupBy("day_of_week").count().orderBy("count", ascending=False)
    
    # Average title length by source
    avg_title_length = df.groupBy("normalized_source") \
        .agg(avg(length(df.title)).alias("avg_title_length")) \
        .orderBy("avg_title_length", ascending=False)
    
    return source_counts, day_counts, avg_title_length

def save_results(source_counts, day_counts, avg_title_length):
    # Save results to MySQL
    source_counts.write \
        .format("jdbc") \
        .option("url", f"jdbc:mysql://{os.getenv('DB_HOST')}/{os.getenv('DB_NAME')}") \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .option("dbtable", "article_source_counts") \
        .option("user", os.getenv('DB_USER')) \
        .option("password", os.getenv('DB_PASSWORD')) \
        .mode("overwrite") \
        .save()

    day_counts.write \
        .format("jdbc") \
        .option("url", f"jdbc:mysql://{os.getenv('DB_HOST')}/{os.getenv('DB_NAME')}") \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .option("dbtable", "article_day_counts") \
        .option("user", os.getenv('DB_USER')) \
        .option("password", os.getenv('DB_PASSWORD')) \
        .mode("overwrite") \
        .save()

    avg_title_length.write \
        .format("jdbc") \
        .option("url", f"jdbc:mysql://{os.getenv('DB_HOST')}/{os.getenv('DB_NAME')}") \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .option("dbtable", "avg_title_length_by_source") \
        .option("user", os.getenv('DB_USER')) \
        .option("password", os.getenv('DB_PASSWORD')) \
        .mode("overwrite") \
        .save()

def main():
    spark = create_spark_session()
    df = load_data_from_mysql(spark)
    processed_df = process_articles(df)
    source_counts, day_counts, avg_title_length = analyze_articles(processed_df)
    save_results(source_counts, day_counts, avg_title_length)
    spark.stop()

if __name__ == "__main__":
    main()